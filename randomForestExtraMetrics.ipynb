{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Directory: C:\\Users\\Greg (perhaps)\\Documents\\Assignment 2 AI\\Assignment 2\n",
      "SPLITTING INTO TRAIN AND VALIDATION\n",
      "TRANSFORMING IMAGES FOR RESNET\n",
      "LOADING RESNET PRETRAINED MODEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXTRACTING FEATURES\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230/230 [07:57<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXTRACTING FEATURES\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [02:39<00:00,  2.07s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    mean_squared_error\n",
    ")\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Set the current working directory\n",
    "BASE_DIR = Path('.').resolve()  # Current working directory\n",
    "print(f\"Base Directory: {BASE_DIR}\")\n",
    "\n",
    "# Define paths relative to the current working directory\n",
    "DATA_DIR = BASE_DIR / 'datasets' / 'dataset'\n",
    "\n",
    "TRAIN_IMAGES_DIR = DATA_DIR / 'images' / 'train'\n",
    "VAL_IMAGES_DIR = DATA_DIR / 'images' / 'val'\n",
    "TEST_IMAGES_DIR = DATA_DIR / 'images' / 'test'\n",
    "\n",
    "TRAIN_LABELS_DIR = DATA_DIR / 'labels' / 'train'\n",
    "VAL_LABELS_DIR = DATA_DIR / 'labels' / 'val'\n",
    "TEST_LABELS_DIR = DATA_DIR / 'labels' / 'test'\n",
    "\n",
    "# Load train and test CSV files from the current working directory\n",
    "train_csv_path = BASE_DIR / 'Train.csv'\n",
    "test_csv_path = BASE_DIR / 'Test.csv'\n",
    "sample_submission_csv_path = BASE_DIR / 'SampleSubmission.csv'\n",
    "\n",
    "train = pd.read_csv(train_csv_path)\n",
    "test = pd.read_csv(test_csv_path)\n",
    "ss = pd.read_csv(sample_submission_csv_path)\n",
    "\n",
    "def visualize_boxes(image_tensor, bboxes, labels=None, class_names=None, image_name=None):\n",
    "    \"\"\"\n",
    "    Visualize bounding boxes on an image tensor.\n",
    "\n",
    "    Parameters:\n",
    "    - image_tensor (torch.Tensor): Image tensor of shape [C, H, W].\n",
    "    - bboxes (np.array): Array of bounding boxes, with each box in (xmin, ymin, xmax, ymax) format.\n",
    "    - labels (np.array, optional): Array of class labels corresponding to the bounding boxes.\n",
    "    - class_names (dict, optional): Mapping of label indices to class names.\n",
    "    - image_name (str, optional): Name of the image to display as title.\n",
    "    \"\"\"\n",
    "    print(f\"Visualizing: {image_name}\")\n",
    "    # Unnormalize the image\n",
    "    image = image_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "    image = image * np.array([0.229, 0.224, 0.225])  # Multiply by std\n",
    "    image = image + np.array([0.485, 0.456, 0.406])  # Add mean\n",
    "    image = np.clip(image, 0, 1)\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # Plot each bounding box\n",
    "    for i, bbox in enumerate(bboxes):\n",
    "        xmin, ymin, xmax, ymax = bbox\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "\n",
    "        # Create a rectangle patch\n",
    "        rect = patches.Rectangle((xmin, ymin), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "\n",
    "        # Add the rectangle to the plot\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Add a label if available\n",
    "        if labels is not None and class_names is not None:\n",
    "            # Reverse class_mapper to get class names from class IDs\n",
    "            inv_class_mapper = {v: k for k, v in class_mapper.items()}\n",
    "            label = inv_class_mapper[labels[i]] if labels[i] in inv_class_mapper else str(labels[i])\n",
    "            plt.text(xmin, ymin - 5, label, color='yellow', fontsize=12, weight='bold')\n",
    "\n",
    "    # Set the title to image name\n",
    "    if image_name:\n",
    "        plt.title(image_name, fontsize=14)\n",
    "\n",
    "    # Display the image with bounding boxes\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Add an image_path column\n",
    "def get_image_path(row):\n",
    "    image_id = row['Image_ID']\n",
    "    if os.path.exists(TRAIN_IMAGES_DIR / image_id):\n",
    "        return TRAIN_IMAGES_DIR / image_id\n",
    "    elif os.path.exists(VAL_IMAGES_DIR / image_id):\n",
    "        return VAL_IMAGES_DIR / image_id\n",
    "    elif os.path.exists(TEST_IMAGES_DIR / image_id):\n",
    "        return TEST_IMAGES_DIR / image_id\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Image {image_id} not found in train, val, or test directories.\")\n",
    "\n",
    "# Add image_path column to training\n",
    "train['image_path'] = train.apply(get_image_path, axis=1)\n",
    "# Add image_path column to testing\n",
    "test['image_path'] = test.apply(get_image_path, axis=1)\n",
    "\n",
    "# Map string classes to integer IDs (encoding)\n",
    "class_mapper = {x: y for x, y in zip(sorted(train['class'].unique().tolist()), range(train['class'].nunique()))}\n",
    "train['class_id'] = train['class'].map(class_mapper)\n",
    "\n",
    "# Drop the 'confidence' column if not needed (since confidence is always 1)\n",
    "train = train.drop(columns=['confidence'])\n",
    "\n",
    "# Split data into training and validation sets\n",
    "print(\"SPLITTING INTO TRAIN AND VALIDATION\")\n",
    "# Drop all duplicate records if they exist\n",
    "train_unique_imgs_df = train.drop_duplicates(subset=['Image_ID'], ignore_index=True)\n",
    "# Split into train and validation sets\n",
    "X_train_ids, X_val_ids = train_test_split(\n",
    "    train_unique_imgs_df['Image_ID'],\n",
    "    test_size=0.25,\n",
    "    stratify=train_unique_imgs_df['class'],\n",
    "    random_state=42\n",
    ")\n",
    "# Setting the training dataframes\n",
    "X_train = train[train.Image_ID.isin(X_train_ids)]\n",
    "X_val = train[train.Image_ID.isin(X_val_ids)]\n",
    "\n",
    "# Define transformations for the images\n",
    "print(\"TRANSFORMING IMAGES FOR RESNET\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224 as required by ResNet\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize as per ResNet requirements\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def resize_bounding_boxes(bboxes, original_width, original_height, new_width=224, new_height=224):\n",
    "    x_scale = new_width / original_width\n",
    "    y_scale = new_height / original_height\n",
    "    resized_bboxes = bboxes.copy()\n",
    "    resized_bboxes[:, 0] = bboxes[:, 0] * x_scale  # Scale xmin\n",
    "    resized_bboxes[:, 1] = bboxes[:, 1] * y_scale  # Scale ymin\n",
    "    resized_bboxes[:, 2] = bboxes[:, 2] * x_scale  # Scale xmax\n",
    "    resized_bboxes[:, 3] = bboxes[:, 3] * y_scale  # Scale ymax\n",
    "    return resized_bboxes\n",
    "\n",
    "# Define a Custom Dataset Class for Loading Images and Annotations\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, transforms=None):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.transforms = transforms\n",
    "        self.image_ids = self.dataframe['Image_ID'].unique()\n",
    "        self.image_data = self.dataframe.groupby('Image_ID')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        records = self.image_data.get_group(image_id)\n",
    "        image_path = records.iloc[0]['image_path']\n",
    "        image = Image.open(str(image_path)).convert(\"RGB\")\n",
    "        original_width, original_height = image.size\n",
    "        bboxes = records[['xmin', 'ymin', 'xmax', 'ymax']].values.astype(np.float32)\n",
    "        labels = records['class_id'].values.astype(np.int64)\n",
    "        bboxes = resize_bounding_boxes(bboxes, original_width, original_height, 224, 224)\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        return image, bboxes, labels\n",
    "\n",
    "# Processing for each batch\n",
    "def custom_collate_fn(batch):\n",
    "    images = torch.stack([item[0] for item in batch], dim=0)\n",
    "    bboxes_batch = [item[1] for item in batch]\n",
    "    labels_batch = [item[2] for item in batch]\n",
    "    return images, bboxes_batch, labels_batch\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CustomDataset(dataframe=X_train, transforms=transform)\n",
    "val_dataset = CustomDataset(dataframe=X_val, transforms=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False, num_workers=0, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Load pre-trained ResNet18 and define feature extractor\n",
    "print(\"LOADING RESNET PRETRAINED MODEL\")\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "resnet18.eval()\n",
    "\n",
    "# Remove the final classification layer, leaving us with image features\n",
    "feature_extractor = nn.Sequential(*list(resnet18.children())[:-1])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "feature_extractor.to(device)\n",
    "\n",
    "# Function to extract features using ResNet\n",
    "def extract_features(data_loader):\n",
    "    print(\"EXTRACTING FEATURES\")\n",
    "    features_list, labels_list, bboxes_list = [], [], []\n",
    "    for images, bboxes_batch, labels_batch in tqdm(data_loader):\n",
    "        images = images.to(device)\n",
    "        with torch.no_grad():\n",
    "            # Extract features\n",
    "            features = feature_extractor(images)\n",
    "            # Flatten to [batch_size, 2048]\n",
    "            features = features.view(features.size(0), -1)\n",
    "        for i in range(len(features)):\n",
    "            num_objects = len(labels_batch[i])\n",
    "            features_list.extend([features[i].cpu().numpy()] * num_objects)\n",
    "            labels_list.extend(labels_batch[i])\n",
    "            bboxes_list.extend(bboxes_batch[i])\n",
    "    return np.array(features_list), np.array(labels_list), np.array(bboxes_list)\n",
    "\n",
    "# Train models and save features\n",
    "X_train_features, y_train_labels, y_train_bboxes = extract_features(train_loader)\n",
    "X_val_features, y_val_labels, y_val_bboxes = extract_features(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FITTING FOR CLASSES AND BOXES\n",
      "PREDICTIONS\n",
      "Validation Classification Accuracy: 0.6329\n",
      "Validation Bounding Box MSE: 2305.7310\n",
      "Weighted Precision: 0.6175\n",
      "Weighted Recall: 0.6329\n",
      "Weighted F1 Score: 0.5954\n",
      "Confusion Matrix:\n",
      "[[1459    0   26    0  115    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0]\n",
      " [ 311   73    3    0   55    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0]\n",
      " [ 102    0  306    0    7    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0   25    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0]\n",
      " [  11    0    0    0  793    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0  353    0    0    3    3    0    2   29   74\n",
      "     0    0    0    0    0    8    0    0    4]\n",
      " [   3    0    0    0    0   32   28    0    5    9    0    3   11   13\n",
      "     0    0    0    0    0    7    0    0   10]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    9    0    0    6]\n",
      " [   0    0    0    0    0    9    0    0   71   10    0    0   15   14\n",
      "     0    0    0    0    0    0    0    0    4]\n",
      " [   0    0    0    0    0    0    0    0    1  143    0    0   12   26\n",
      "     0    0    0    0    0    0    0    0    2]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    2    4\n",
      "     0    0   39    0    0   68    0    0   22]\n",
      " [   0    0    0    0    0   18    0    0    0    0    0   51   14   23\n",
      "     0    0    2    0    3   10    0    0    0]\n",
      " [   0    0    0    0    0   11    0    0    0    9    0    0  270   70\n",
      "     0    0    0    0    1    4    0    0    9]\n",
      " [   0    0    0    0    0   52    0    0    1   12    0    2   65  391\n",
      "     0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    1    0    0   44    0    0    0   13   48\n",
      "     0    0   25    0   40  220    0    0  306]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0   13    0\n",
      "     0    5    2    0    0   35    0    0   64]\n",
      " [   0    0    0    0    0   13    0    0    0    0    0    0   30    1\n",
      "     0    2  267    1   16   33    0    0  113]\n",
      " [  34    0    0    0    0   16    0    0   10    0    0    0    0    0\n",
      "     0    0    0  174    0   24    0    0   10]\n",
      " [   0    0    0    0    0   27    0    0    0    0    0    0   28   10\n",
      "     0    0    3    0  524    0    0    0  155]\n",
      " [   7    0    0    0    0    0    0    0    6   12    0    0   40   21\n",
      "     0    0   75    6    0  495    0    0  212]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0   54    0\n",
      "     0    0    0    0    4    0   23    0   12]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0   14    0\n",
      "     0    0    0    0    0    0   11    0    7]\n",
      " [   0    0    0    0    0    1    0    0   44    0    0    0   90   53\n",
      "     0    0   36    0   52  249    0    0 1063]]\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "Corn_Cercospora_Leaf_Spot       0.76      0.91      0.83      1600\n",
      "         Corn_Common_Rust       1.00      0.17      0.28       442\n",
      "             Corn_Healthy       0.91      0.74      0.82       415\n",
      "Corn_Northern_Leaf_Blight       0.00      0.00      0.00        25\n",
      "              Corn_Streak       0.80      0.99      0.88       804\n",
      "    Pepper_Bacterial_Spot       0.66      0.74      0.70       476\n",
      "        Pepper_Cercospora       1.00      0.23      0.38       121\n",
      "      Pepper_Early_Blight       0.00      0.00      0.00        15\n",
      "          Pepper_Fusarium       0.38      0.58      0.46       123\n",
      "           Pepper_Healthy       0.72      0.78      0.75       184\n",
      "       Pepper_Late_Blight       0.00      0.00      0.00       135\n",
      "       Pepper_Leaf_Blight       0.88      0.42      0.57       121\n",
      "         Pepper_Leaf_Curl       0.39      0.72      0.50       374\n",
      "       Pepper_Leaf_Mosaic       0.52      0.75      0.62       523\n",
      "          Pepper_Septoria       0.00      0.00      0.00       697\n",
      "    Tomato_Bacterial_Spot       0.71      0.04      0.08       119\n",
      "      Tomato_Early_Blight       0.59      0.56      0.58       476\n",
      "          Tomato_Fusarium       0.96      0.65      0.78       268\n",
      "           Tomato_Healthy       0.82      0.70      0.76       747\n",
      "       Tomato_Late_Blight       0.43      0.57      0.49       874\n",
      "         Tomato_Leaf_Curl       0.68      0.25      0.36        93\n",
      "            Tomato_Mosaic       0.00      0.00      0.00        32\n",
      "          Tomato_Septoria       0.53      0.67      0.59      1588\n",
      "\n",
      "                 accuracy                           0.63     10252\n",
      "                macro avg       0.55      0.45      0.45     10252\n",
      "             weighted avg       0.62      0.63      0.60     10252\n",
      "\n",
      "SAVING TO CSV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [02:19<00:00,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"FITTING FOR CLASSES AND BOXES\")\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train_features, y_train_labels)\n",
    "reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "reg.fit(X_train_features, y_train_bboxes)\n",
    "\n",
    "print(\"PREDICTIONS\")\n",
    "y_val_pred_labels = clf.predict(X_val_features)\n",
    "y_val_pred_bboxes = reg.predict(X_val_features)\n",
    "\n",
    "# Calculate and print accuracy and MSE\n",
    "class_accuracy = accuracy_score(y_val_labels, y_val_pred_labels)\n",
    "bbox_mse = mean_squared_error(y_val_bboxes, y_val_pred_bboxes)\n",
    "print(f'Validation Classification Accuracy: {class_accuracy:.4f}')\n",
    "print(f'Validation Bounding Box MSE: {bbox_mse:.4f}')\n",
    "\n",
    "# **NEW CODE FOR METRICS**\n",
    "\n",
    "# Calculate weighted accuracy\n",
    "# Note: In multi-class classification, accuracy is already a global metric.\n",
    "weighted_accuracy = class_accuracy  # Since accuracy accounts for all classes\n",
    "\n",
    "# Calculate weighted precision, recall, and F1 score\n",
    "weighted_precision = precision_score(y_val_labels, y_val_pred_labels, average='weighted', zero_division=0)\n",
    "weighted_recall = recall_score(y_val_labels, y_val_pred_labels, average='weighted', zero_division=0)\n",
    "weighted_f1 = f1_score(y_val_labels, y_val_pred_labels, average='weighted', zero_division=0)\n",
    "\n",
    "print(f'Weighted Precision: {weighted_precision:.4f}')\n",
    "print(f'Weighted Recall: {weighted_recall:.4f}')\n",
    "print(f'Weighted F1 Score: {weighted_f1:.4f}')\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_val_labels, y_val_pred_labels)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "# Optionally, display classification report\n",
    "class_report = classification_report(y_val_labels, y_val_pred_labels, target_names=class_mapper.keys(), zero_division=0)\n",
    "print('Classification Report:')\n",
    "print(class_report)\n",
    "\n",
    "# **END OF NEW CODE**\n",
    "\n",
    "# Save predictions to CSV\n",
    "print(\"SAVING TO CSV\")\n",
    "predictions = []\n",
    "for i, (images, bboxes_batch, labels_batch) in enumerate(tqdm(val_loader)):\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        # Extract features\n",
    "        features = feature_extractor(images)\n",
    "        # Flatten for random forest\n",
    "        features = features.view(features.size(0), -1).cpu().numpy()\n",
    "    predicted_labels = clf.predict(features)\n",
    "    confidences = clf.predict_proba(features).max(axis=1)\n",
    "    predicted_bboxes = reg.predict(features)\n",
    "    for idx in range(len(images)):\n",
    "        image_id = val_dataset.image_ids[i * len(images) + idx]\n",
    "        class_label = list(class_mapper.keys())[list(class_mapper.values()).index(predicted_labels[idx])]\n",
    "        for j in range(len(bboxes_batch[idx])):\n",
    "            confidence = confidences[idx]\n",
    "            ymin, xmin, ymax, xmax = predicted_bboxes[idx]\n",
    "            predictions.append({\n",
    "                \"Image_ID\": image_id,\n",
    "                \"class\": class_label,\n",
    "                \"confidence\": confidence,\n",
    "                \"ymin\": ymin,\n",
    "                \"xmin\": xmin,\n",
    "                \"ymax\": ymax,\n",
    "                \"xmax\": xmax\n",
    "            })\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df.to_csv(\"predictions.csv\", index=False)\n",
    "print(\"Predictions saved to predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
